{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5292a3e4-7b4e-4617-9dac-2d206419b873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81870d22-acda-4bbe-b281-a804da8964c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07191537-aff7-4c24-ab0d-152390f67412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reset_or_init_directories(reset_all):\n",
    "  for i in [transaction_output, dimension_output]:\n",
    "    try:\n",
    "        dbutils.fs.ls(f\"{i}\")\n",
    "        if reset_all:\n",
    "          print(f\"deleting {i}\")\n",
    "          dbutils.fs.rm(i, True)\n",
    "          print(f\"recreating {i}\")\n",
    "          dbutils.fs.mkdirs(i)\n",
    "        else: \n",
    "          print(\"no data to delete\")\n",
    "    except:\n",
    "        print(f\"making {i}\")\n",
    "        dbutils.fs.mkdirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7ee476-20d4-4fc2-b5bf-6bf0af75d673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_tables_and_checkpoints(*,tables=[''],catalog='',schema='',checkpoint_dir=''):\n",
    "  for i in tables:\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.{i}\")\n",
    "        print(f\"successfully dropped {catalog}.{schema}.{i}\")\n",
    "        dbutils.fs.rm(f\"{checkpoint_dir}/{catalog}.{schema}.{i}\", True)\n",
    "        print(f\"{checkpoint_dir}/{catalog}.{schema}.{i}\")\n",
    "        print(f\"successfully removed checkpoint directory for {catalog}.{schema}.{i}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbfeb95-b4b9-404f-9529-8ce15716b579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_stream_data(*,chunk_size=1000):\n",
    "  fake = Faker()\n",
    "\n",
    "  # List of stock tickers\n",
    "  tickers = [\"AAPL\", \"GOOGL\", \"AMZN\", \"FB\", \"NFLX\",\"NVDA\",\"VOO\"]\n",
    "\n",
    "  buyers = [\"Vanguard\", \"BlackRock\", \"State Street Global Advisors\", \"Fidelity Investments\",\n",
    "                          \"Capital Research and Management Company\", \"The Vanguard Group\",\n",
    "                          \"T. Rowe Price\", \"Bank of New York Mellon\", \"JPMorgan Chase\", \n",
    "                          \"Goldman Sachs\", \"BNP Paribas Asset Management\", \"Northern Trust Corporation\",\"Bank of Andrew\"]\n",
    "\n",
    "  sellers = [\"JPMorgan Chase\", \"Bank of America\", \"Citigroup\", \"Wells Fargo\", \"Goldman Sachs\",\n",
    "                      \"Morgan Stanley\", \"Barclays\", \"HSBC Holdings\", \"BNP Paribas\", \"UBS Group\", \"Credit Suisse\", \"Deutsche Bank\"]\n",
    "  \n",
    "\n",
    "  # Generate transaction data\n",
    "  transactions = []\n",
    "  for _ in range(chunk_size):\n",
    "      ticker = random.choice(tickers)\n",
    "      buyer = random.choice(buyers)\n",
    "      seller = random.choice(sellers)\n",
    "      timestamp = fake.date_time_between_dates(datetime_start='-1d', datetime_end='now')\n",
    "      price = round(random.uniform(100, 1000), 2)\n",
    "      volume = random.randint(100, 1000)\n",
    "      transactions.append((timestamp, buyer,seller, ticker, price, volume))\n",
    "\n",
    "  # Convert transactions list to a DataFrame\n",
    "  df = pd.DataFrame(transactions, columns=[\"timestamp\", \"buyer\", \"seller\", \"ticker\", \"price\", \"volume\"])\n",
    "\n",
    "  output_path = f\"{transaction_output}/transactions_{int(time.time())}.csv\"\n",
    "  # Write DataFrame to CSV\n",
    "  df.to_csv(output_path, index=False)\n",
    "  \n",
    "  return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98e124e-20d2-4b17-ad28-fb1ab5795285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_a_stream( *, total_rows_m=1, stream_length_m=5, interval_s=3, reset_data=False):\n",
    "  \"\"\"\n",
    "  This function will build a stream of data to a directory over a given timeframe, and log the output\n",
    "  It will first check to see if all existing data, and checkpoints should be deleted and recreated\n",
    "  \"\"\"\n",
    "\n",
    "  intervals_count = int(stream_length_m * 60 // interval_s)\n",
    "  batch_size = int((total_rows_m/intervals_count)*1000000)\n",
    "\n",
    "  output_log = {\"intervals_count\": intervals_count, \"batch_size\":batch_size}\n",
    "  \n",
    "  #considered for later date \n",
    "  # input(f\"this stream will take {stream_length_m} minutes, produce {intervals_count} files, and {batch_size} rows per csv every {interval_s} seconds: Continue(Y/N):\")\n",
    "  \n",
    "  print(f\"Stream Metrics:\\n\"\n",
    "        f\"----------------\\n\"\n",
    "        f\"Duration: {stream_length_m:.2f} minutes\\n\"\n",
    "        f\"Intervals: {intervals_count}\\n\"\n",
    "        f\"Batch Size: {batch_size} rows per csv\\n\"\n",
    "        f\"---BEGINNING STREAM----------------\\n\")\n",
    "\n",
    "  reset_or_init_directories(reset_data)\n",
    "  \n",
    "  for i in range(1,intervals_count):\n",
    "      \n",
    "      file = write_stream_data(chunk_size=batch_size)\n",
    "      print(f\"wrote: {i} of {intervals_count} files\")\n",
    "      time.sleep(interval_s)\n",
    "\n",
    "  return output_log"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "0_Main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
